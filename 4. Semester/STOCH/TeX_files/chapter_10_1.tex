\section{Die Verteilungskonvergenz}
\begin{definition}[\begriff{schwache Konvergenz}]
	$Y, Y_1, Y_2, \dots$ reelle Zufallsvariablen (``jede ZV darf ihren eigenen WR mitbringen''). Falls für alle $f \in C_b(\R)$ (stetig und beschränkt) gilt, dass
	\begin{align*}
		\lim_{n\to \infty} \E[f(Y_n)] = \E[f(Y)]
	\end{align*} %TODO set \begriff!
	so \emph{konvergiert} $(Y_n)_{n \in \N}$ schwach / in Verteilung gegen $Y$.\\
	Schreibe: $Y_n \konverteil Y$ oder $Y_n \Rightarrow Y, n \to \infty$ oder $\P_{Y_n} \Rightarrow \P_Y$.
\end{definition}
\begin{*remark}
	\begin{itemize}
		\item Formal sollten wir eigentlich schreiben
		\begin{align*}
			\lim \E_n[f(Y_n)] = \E[f(Y)]
		\end{align*}
		wobei $\E_n[f(Y_n)]$ bzgl. $\P_n, Y_n$ auf $(\O_n,\F_n,\P_n)$. 
		Dies wird aber in der Regeln vernachlässigt.
		\item Für Zufallsvariablen in $\Rd$ lässt sich schwache Konvergenz mittels $f \in C_b(\Rd)$ (d.h. $f: \Rd \to \R$) definieren.
	\end{itemize}
\end{*remark}
Der Grenzwert einer schwach konvergenten Folge ist eindeutig \emph{in Verteilung}.
\begin{lemma}
	$Y,Z,Y_1, Y_2, \dots$ reelle Zufallsvariablen, so dass
	\begin{align*}
	Y_n \konverteil Y \und Y_n \konverteil Z
	\end{align*}
	Dann gilt: $Y \distri Z$ bzw. $P_Y = \P_Z$.
\end{lemma}
\begin{proof}
	Betrachte ein fixes kompaktes Intervall $[a,b] \subset \R$. Da die kompakten Intervalle ein $\cap$-stabiler Erzeuger von $\borel(\R)$ sind, genügt es zu zeigen
	\begin{align*}
		\int \indi_{[a,b]} \d \P_Y = \P_Y([a,b]) = \P_Z([a,b]) = \int \indi_{[a,b]} \d \P_Z.
	\end{align*}
	Dazu konstruiere eine Folge $(f_k)_{k \in \N}$ in $C_b(\R)$, so dass $f_k \downarrow f = \indi_{[a,b]}$ ($\nearrow$ Beweis zu \cref{8_13}). Dann folgt mit monotoner Konvergenz
	\begin{align*}
		\int \indi_{[a,b]} \d \P_Y = \lim_{k \to \infty} \int f_k \d \P_Y = \lim_{k \to \infty} \lim_{n \to \infty} \int f_k \d \P_{Y_n}
		\intertext{und analog}
		\int \indi_{[a,b]} \d \P_Z = \lim_{k \to \infty} \lim_{n \to \infty} \int f_k \d \P_{Y_n}
	\end{align*}
	Das liefert die Behauptung.
\end{proof}
\begin{proposition}[\person{Portmanteau}]
	$Y,Y_1,Y_2, \dots$ reelle Zufallsvariablen. Die folgenden Aussagen sind äquivalent:
	\begin{enumerate}
		\item $Y_n \konverteil Y$
		\item $\lim_{n \to \infty} \E[f(Y_n)] = \E[f(Y)] \quad \forall f \in C_b^g(\R)$ (glm stetig und beschränkt)
		\item $\limsup_{n \to \infty} \P(Y_n \in F) \le \P(Y \in F) \quad \forall F \subset \R$ abgeschlossen
		\item $\liminf_{n \to \infty} \P(Y_n \in O) \ge \P(Y \in O) \quad \forall O \subset \R$ offen
		\item $\lim_{n \to \infty} \P(Y_n \in C) = \P(Y \in C) \quad C \in \borel(\R)$ mit $\P_Y(\partial C) = 0$ (Rand von $C$)
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}[label=] %TODO add labels
		\item 1. $\implies$ 2.: ist klar
		\item 2. $\implies$ 3.: Sei $F$ abgeschlossen und definiere für $k \in \N$
		\begin{align*}
			f_k(x) = (1-k \dist (x,F))^+ \mit \dist(x,F) = \inf_{y \in F} \abs{x-y}
		\end{align*}
		Dann ist $f_k$ beschränkt und glm. stetig, denn
		\begin{align*}
			\abs{f_k(y) - f_k(x)} &\le k \abs{\dist(y,F) - \dist(x,F)}\\
			&\le k \abs{y-x} \quad \forall x,y \in \R
			\intertext{da}
			\dist(x,F) &= \inf_z \abs{x-z} \le \inf_z (\abs{x-z} + \abs{y-z})\\
			&= \abs{x-y} + \dist{y,F}.
		\end{align*}
		Zudem gilt $f_k \le \indi_F$ und $f_k \downarrow \indi_F$, so dass
		\begin{align*}
			\limsup_{n \to \infty} \P(Y_n \in F) &= \limsup_{n \to \infty} \E[\indi_F (Y_n)]\\
			&\le \lim_{n \to \infty} \E[f_k(Y_n)]\\
			\over{\text{2.}}&{=} \E[f_k (Y)]
		\end{align*}
		Mit monotoner Konvergenz folgt
		\begin{align*}
			\limsup_{n \to \infty} \P(Y_n \in F) &\le \inf_{k \in \N} \E[f_k(Y)]\\
			&= \E[\indi_F(Y)] = \P(Y \in F)
		\end{align*}
		\item 3. $\implies$ 4.: Für jedes $O \subset \R$ offen ist $O^C$ abgeschlossen, so dass
		\begin{align*}
			\liminf_{n \to \infty} \P(Y_n \in O) &= \liminf_{n \to \infty} (1-\P(Y_n \in O^C))\\
			&= 1- \limsup_{n \to \infty} \P(Y_n \in O^C)\\
			\over{\text{3.}}&{\ge} 1 - \P(Y \in O^C)\\
			&= \P(Y \in O).
		\end{align*}
		\item 4. $\implies$ 3.: Analog und vertausche $\limsup$ mit $\liminf$.
		\item 4. und 3. $\implies$ 5.: Sei $C \in \borel(\R)$ und $overset{\circ}{C}$ dass offene Innere von $C$, $\bar{C} = \overset{\circ}{C} \cup \partial C$ der Abschluss.
		\begin{align*}
			\lim_{n \to \infty} \P(Y_n \in C) &\le \limsup_{n \to \infty} \P(Y_n \in \bar{C})\\
			\overset{\text{3.}}&{\le} \P(Y \in \bar{C})\\
			&= \P(Y \in \overset{\circ}{C}) \quad (\text{ da } \P_Y(\partial C) = 0)\\
			\overset{\text{4.}}&{\le} \liminf_{n \to \infty} \P(Y_n \in \overset{\circ}{C})\\
			&\le \limsup_{n \to \infty} \P(Y_n \in C).
		\end{align*}
		\item 5. $\implies$ 1.: Sei $f \in C_b(\R)$ positiv. (wenn nicht positiv: in positiven und negativen Anteil zerlegen und dann mit Linearität arbeiten). Da $\partial \set{f \ge t}  = \set{f=t}$ gilt, folgt $\P_Y(\partial\set{f \ge t}) > 0$ für höchstens abzählbar viele $t$ und das impliziert 
		\begin{align*}
			\lim_{n \to \infty} \E[f(Y_n)] &= \lim_{n\to \infty} \int_{\R} f \d \P_{Y_n}\\
			&= \lim_{n \to \infty} \int_0^{\infty} \P_{Y_n} (f\ge t)\d t \quad \text{ folgt mit Schilling MINT Satz 16.7}\\
			&= \lim_{n \to \infty} \int_0^{\infty} \P(f(Y_n) \ge t)\d t\\
			&= \int_0^{\infty} \lim_{n \to \infty} \P(f(Y_n) \ge t) \d t \quad \text{dom. Konvergenz}\\
			&= \int_0^{\infty} \P(f(Y) \ge t) \d t \quad \text{ nutze 5.}\\
			&= \E[f(Y)] \quad \text{Satz 16.7}.
		\end{align*}
		Für allgemeines $f$ folgt die Aussage mittels Linearität.
	\end{enumerate}
\end{proof}
\begin{lemma}
	\proplbl{10_4}
	$Y, Y_1, Y_2, \dots$ reelle Zufallsvariablen auf $(\O, \F,\P)$. Es gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\P} \implies Y_n \xrightarrow[n \to \infty]{\d} Y
	\end{align*}
\end{lemma}
\begin{proof}
	Seien $f \in C_b^g(\R)$ und $\epsilon > 0$ fixiert. Betrachte
	\begin{align*}
		\abs{\E[f(Y_n)] - \E[f(Y)]} \le \E[\abs{f(Y_n) - f(Y)}] = \int_{\R} \abs{f(Y_n) - f(Y)}\d \P
	\end{align*}
	Da $f \in C_0^g$ existiert $M \in \R$ mit $\abs{f} \le M$ und 
	\begin{align*}
		\exists \delta = \delta(\epsilon) \in (0,1) \text{ so dass }\forall \abs{x-y}\le \delta \colon \abs{f(x)-f(y)} < \epsilon \tag{$\star$}\label{eq:10_4:a}
	\end{align*}
	Schreibe nun
	\begin{align*}
		\int \abs{f(Y_n)-f(Y)}\d \P = \int_{\set{\abs{Y_n -Y}\le \delta}} ... \D \P + \int_{\set{\abs{Y_n -Y}> \delta}} ... \d \P = E_1 +E_2
	\end{align*}
	mit
	\begin{align*}
		E_1 \overset{\eqref{eq:10_4:a}}&{\le}\epsilon \P(\abs{Y_n - Y} \le \delta)\\
		E_2 &\le 2M \P(\abs{Y_n - Y} > \delta)\\
		E_1 + E_2 &\le \epsilon \P(\abs{Y_n - Y} \le \delta) + 2 M \P(\abs{Y_n - Y} > \delta)\\
		&\le \epsilon + 2 M \underbrace{P(\abs{Y_n -Y} > \delta)}_{\xrightarrow[n \to \infty]{}0} \to \epsilon
	\end{align*}
	Für $\epsilon \to 0$ folgt die Behauptung.
\end{proof}
\begin{lemma}
	$Y,Y_1,Y_2,\dots$ reelle Zufallsvariablen auf $(\O,\F,\P)$. Für $c \in \R$ konstant gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\d} Y \equiv c \Longleftrightarrow Y_n \xrightarrow[n \to \infty]{\P} Y \equiv c
	\end{align*}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item $\Leftarrow$: \propref{10_4}
		\item $\Rightarrow$: Für $\epsilon > 0$ fixiert, wähle $f \in C_b(\R)$ so dass
		\begin{align*}
			f(0) = 0 &\quad f(x) \ge \indi_{[-\epsilon,\epsilon]^C}(x) \quad \forall x \in \R
			\intertext{Dann}
			\P(\abs{Y_n - Y}> \epsilon) &= \int_{[-\epsilon,\epsilon]^C} (Y_n - Y)\d \P\\
			&\le \int F(Y_n-Y)\d \P\\
			&= \int f(Y_n - c)\d \P\\
			&\xrightarrow[n\to \infty]{} \int f(Y-c) \d \P\\
			&= \int f(0)\d \P = 0.
		\end{align*}
	\end{itemize}
\end{proof}
\begin{proposition}
	$Y,Y_1,Y_2,\dots$ reelle Zufallsvariablen mit Verteilungsfunktionen $F_Y,F_{Y_1},F_{Y_2}, \dots$.
	Dann gilt $Y_n \xrightarrow[n \to \infty]{\d} Y$ genau dann wenn
	\begin{align*}
		\lim_{n \to \infty} F_{Y_n} = F_Y(x) \quad \forall \text{ Stetigkeitsstellen $x$ von $F_Y$}.
	\end{align*}
	Ist $F_Y$ stetig, so gilt in diesem Fall sogar gleichmäßige Konvergenz
	\begin{align*}
		\lim_{n \to \infty} \sup_{x \in \R} \abs{F_{Y_n}(x) - F_Y(x)} = 0.
	\end{align*}
\end{proposition}
\begin{proof}
	Es gelte $Y_n \xrightarrow[n\to \infty]{\d} Y$, d.h. $\E[f(Y_n)]\to \E[f(Y)]\; \forall f \in C_b(\R)$. Sei $x \in \R$ fixiert, $\epsilon > 0$ und wähle $f \in C_b(\R)$ mit
	\begin{align*}
		\indi_{(-\infty,x]} &\le f \le \indi_{(-\infty,x+\epsilon]}
		\intertext{Dann}
		\limsup_{n \to \infty} F_{Y_n}(x) &= \limsup_{n \to \infty} \P(Y_n \le x)\\
		&= \limsup_{n \to \infty} \E[\indi_{(-\infty,x]} (Y_n)]\\
		&\le \limsup_{n \to \infty} \E[f(Y_n)]\\
		&= \lim \E[f(Y_n)] = \E[f(Y)]\\
		&\le \E[\indi_{(-\infty,x+\epsilon]}(Y)] = F_Y(x+\epsilon)
	\end{align*}
	Für $\epsilon \to 0$ folgt, da $F_Y$ rechtsstetig
	\begin{align*}
		\limsup_{n \to \infty} F_{Y_n} (x) \le F_Y(x)
	\end{align*}
	Ist $x$ Stetigkeitsstelle von $F_Y$, so gilt auch $F_Y(x-\epsilon) \to F_Y(x), \epsilon \to 0$ und eine analoge Rechnung zeigt
	\begin{align*}
		\liminf_{n \to \infty} F_{Y_n}(x) \ge F_Y(x)
		\intertext{also}
		\lim F_{Y_n}(x) = F_Y(x)
	\end{align*}
	Umgekehrt: Es gelte $\lim_{n \to \infty} F_{Y_n}(x) = F_Y(x)$ $\forall$ Stetigkeitsstellen $x$ von $F_Y$. Fixiere $f \in C_b^g(\R)$ (\person{Portmanteau}!), $\epsilon > 0$ und wähle Stetigkeitsstellen $x_1 < x_2 < ... x_n$ von $F_Y$, so dass
	\begin{align*}
		F_Y(x_1) < \epsilon, F_Y(x_k) > 1 -\epsilon \und\\
		\abs{f(y)-f(x_i)} < \epsilon \quad \forall x_{i-1} \le y \le x_i
	\end{align*}
	(möglich, da $f$ gleichmäßig stetig und $F_Y$ wegen Monotonie nur abzählbar viele Unstetigkeitsstellen besitzt.) Das impliziert
	\begin{align*}
		\E[f(Y_n)] &= \sum_{i=2}^k \E[f(y_n)\indi_{\set{x_{i-1} < Y_n \le x_i}}] + \E[f(Y_n)\indi_{\set{Y_n \le x_1}\cup \set{Y_n > x_k}}]\\
		&\le \sum_{i=2}^k (f(x_i)+\epsilon)(F_{Y_n}(x_i) - F_{Y_n}(x_{i-1})) + 2\epsilon\norm{f}_{\infty}\\
		&\xrightarrow[n \to \infty]{} \sum_{i=2}^k (f(x_i)+ \epsilon)(F_Y(x_i)-F_Y(x_{i-1}) + 2\epsilon\norm{f}_{\infty}\\
		&\le \E[f(Y)] + 2\epsilon(1+2\norm{f}_{\infty})
	\end{align*}
	Für $\epsilon \to 0$ folgt
	\begin{align*}
		\limsup_{n \to \infty} \E[f(Y_n)] \le \E[f(Y)]
	\end{align*}
	Der $\liminf$ folgt analog
	\begin{align*}
		\lim \E[f(Y_n)] = \E[f(Y)].
	\end{align*}
	Zur gleichmäßigen Konvergenz: Sei $F_Y$ stetig und $\epsilon = k^{-1}, k \in \N$. Dann existieren $z_i \in \R$ mit
	\begin{align*}
		F_Y(z_i) = \frac{i}{k} \quad 0 < i < k
	\end{align*}
	Da auch $F_{Y_n}$ monoton wächst, gilt
	\begin{align*}
		\sup_{x \in \R} \abs{F_{Y_n}(x) -F_Y(x)} \le \epsilon + \underbrace{\max_{0 \le i \le k}\abs{F_{Y_n}(z_i)-F_Y(z_i)}}_{\xrightarrow[n \to \infty]{}0}.
	\end{align*}
	%TODO tikzpic check ERIC!
\end{proof}
\begin{proposition}[Stetigkeitssatz]
	\proplbl{10_7}
	$Y, Y_1, Y_2, \dots$ Zufallsvariablen in $\Rd$ mit charakteristischen Funktionen $\varphi_Y, \varphi_{Y_1}, \varphi_{Y_2}, \dots$. Dann gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\d} Y \Longleftrightarrow \lim_{n \to \infty} \varphi_{Y_n}(u) = \varphi_Y(u) \quad \forall u \in \Rd
	\end{align*} 
\end{proposition}
Für den Beweis benötigen wir:
\begin{lemma}
	\proplbl{10_8}
	$Y,Y_1,Y_2, \dots$ Zufallsvariablen in $\Rd$ und sei $N = (N_1, \dots, N_d)^T$ Zufallsvektor so dass $N_1, N_2, \dots, N_d$ u.i.v. mit $N_i \sim \normal(0,1)$, unabhängig von $Y,Y_1, Y_2, \dots$ Falls:
	\begin{align*}
		Y_n + \sigma N \xrightarrow[n \to \infty]{\d} Y + \sigma N \quad \sigma > 0
		\intertext{dann folgt}
		Y_n \xrightarrow[n \to \infty]{\d} Y
	\end{align*}
\end{lemma}
\begin{proof}
	Sei $f \in C_b^g(\R)$ und $\epsilon > 0$ beliebig.
	\begin{align*}
		\abs{\E[f(Y_n)-f(Y)]} &\le \abs{\E[f(Y_n)] - \E[f(Y_n + \sigma N)]}\\
		&+ \underbrace{\abs{\E[f(Y_n + \sigma N)] - \E[f(Y+\sigma N)]}}_{\xrightarrow[n \to \infty]{}0}\\
		&+ \abs{\E[f(Y+\sigma N)] - \E[f(Y)]}
	\end{align*}
	Da $f \in C_b^g(\R)$ existiert $\delta > 0$, so dass
	\begin{align*}
		\abs{f(x)-f(y)} \le \epsilon \text{ für } \abs{x-y} \le \delta
	\end{align*}
	Damit
	\begin{align*}
		\abs{\E[f(Y_n)] - \E[f(Y_n + \sigma N)]} &= \abs{\E[\underbrace{f(Y_n) - f(Y_n + \sigma N)}_{\le \epsilon \mit \norm{\sigma N}< \delta}]}\\
		&\le \epsilon + 2 \norm{f}_{\infty}\P(\norm{N} > \frac{\delta}{\sigma}) \le 2 \epsilon \quad \text{ für } \sigma \le \sigma_0(\delta, \epsilon), \forall n \in \N
	\end{align*}
	Dann folgt für $n$ hinreichend groß $\abs{\E[f(Y_n)] - \E[f(Y)]} \le 5 \epsilon$. Daraus folgt die Behauptung.
\end{proof}
\begin{proof}[\propref{10_7}]
	Es gilt $Y_n \xrightarrow[n \to \infty]{\d} Y$, dann folgt
	\begin{align*}
		\varphi_{Y_n}(u) &= \E[e^{\ii \scaProd{u}{Y_n}}]= \E[\cos(\scaProd{u}{Y_n})] + \ii \E[\sin(\scaProd{u}{Y_n})]\\
		&\xrightarrow[n \to \infty]{} \E[\cos(\scaProd{u}{Y})] + \ii \E[\sin(\scaProd{u}{Y})]\\
		&= \varphi_Y (u).
	\end{align*}
	Gelte umgekehrt $\varphi_{Y_n}(u) \xrightarrow{} \varphi_Y(u)\forall u \in \Rd$. Nach \propref{10_8} genügt zu zeigen, dass
	\begin{align*}
		Y_n + \sigma N \xrightarrow{\d} Y + \sigma N \quad \sigma > 0
	\end{align*}
	mit $N$ wie in \propref{10_8}. Sei $f \in C_b(\R)$, dann gilt ($\nearrow$ Beweis zu \cref{8_12}).
	\begin{align*}
		\E[f(Y_n + \sigma N)] &= \int_{\Rd} f(z) \rho_{Y_n + \sigma N}(z) \d z\\
		\intertext{mit}
		\rho_{Y_n + \sigma N}(z) &= (2 \pi)^d \int_{\Rd} \varphi_{Y_n}(-y) e^{\ii \scaProd{z}{y}} e^{- \sfrac{\sigma^2 y^2}{2}}\d y
		\intertext{so dass}
		 \abs{\E[f(Y_n + \sigma N)] -\E[f(Y+\sigma N)]} &= \abs{\int_{\Rd} f(z) (\rho_{Y_n + \sigma N}(z) - \rho_{Y + \sigma N}(z)\d z}\\
		 &\le \norm{f}_{\infty}\int_{\Rd} \abs{\rho_{Y_n + \sigma N}(z)-\rho_{Y + \sigma N}(z)}\d z\\
		 &= \norm{f}_{\infty} \int_{\Rd} (\rho_{Y_n + \sigma N}(z) - \rho_{Y + \sigma N}(z))^+ + (\rho_{Y_n + \sigma N}(z)- \rho_{Y + \sigma N}(z))^- \d z\\
		 \overset{\eqref{eq:bew:10_8:a}}&{=} 2 \norm{f}_{\infty} \int_{\Rd} (\rho_{Y_n + \sigma N} - \rho_{Y + \sigma N})^+ \d z\\
		 &\xrightarrow[n \to \infty]{} 0
	\end{align*}
	Majorante: $\rho_{Y_n + \sigma N}(z)$ und $e^{-\sfrac{\sigma^2 y^2}{2}}$
	nach dominierter Konvergenz, da $\varphi_{Y_n}(u) \xrightarrow{} \varphi_Y(u)$.\\
	Zu \propref{eq:bew:10_8:a}:
	\begin{align*}
		&\int_{\Rd} (\rho_{Y_n + \sigma N}(z) - \rho_{Y + \sigma N}(z))^+ \d z - \int_{\Rd} (\rho_{Y_n + \sigma N}(z)- \rho_{Y + \sigma N}(z))^- \d z\\
		&= \int_{\Rd} (\rho_{Y_n + \sigma N} (z) - \rho_{Y + \sigma N}(z)) \d z\\
		&= 1-1 = 0 \label{eq:bew:10_8:a} \tag{$\star$}
	\end{align*}
\end{proof}
Mit dem Stetigkeitssatz lassen sich bekannte Resultate reproduzieren.
\begin{lemma}[\person{Poisson}approximation]
	Sei $X_n \sim \Bin(n,p_n)$, so dass $np_n \xrightarrow[n \to \infty]{}$ dann gilt
	\begin{align*}
		X_n \xrightarrow[n \to \infty]{\d} X \sim \Pois(\lambda).
	\end{align*}
\end{lemma}
\begin{proof}
	Nach \cref{5_15}
	\begin{align*}
		\varphi_{X_n}(u) &= \psi_{X_n}(e^{\ii u}) = (1+p_n(e^{\ii u} - 1))^n\\
		&= (1+ \frac{n p_n(e^{\ii u} - 1)}{n})^n\\
		& \xrightarrow[n \to \infty]{} e^{\lambda(e^{\ii u} - 1)} = \psi_X(u)
	\end{align*}
	und mit dem Stetigkeitssatz folgt die Behauptung.
\end{proof}
\begin{proposition}[WLLN, u.i.v-Version in $\Rd$]
	Seien $X_1,X_2, \dots$ u.i.v $\Rd$-wertige Zufallsvariablen auf $(\O,\F,\P)$ mit $X_i \in \Ln{1}(\P)$. Dann gilt
	\begin{align*}
		1/n \sum_{i=1}^n \xrightarrow[n \to \infty]{\P} \E[X_1].
	\end{align*}
\end{proposition}
\begin{proof}
	Für $u \in \Rd$ gilt
	\begin{align*}
		\varphi_{1/n \sum X_i} (u) &= \varphi_{\sum X_i}(u/n) \\
		\overset{\text{unabh.}}&{=} \prod_{i=1}^n \varphi_{X_i} (u/n) \overset{\text{ident. verteilt}}{=} (\varphi_{X_1}(u/n))^n\\
		\overset{\text{\person{Taylor}}}&{=} (1+ 1/n \scaProd{u}{\varphi'_{X_1}(0)} + o(\norm{u/n}))^n\\
		&= (1 + 1/n \ii \scaProd{u}{\E[X_1]} + o(\norm{u/n}))^n \quad \propref{8_14}\\
		&\xrightarrow[n \to \infty]{} \exp(\ii \scaProd{u}{\E[X_1]})
	\end{align*}
	und dies ist die charakteristische Funktion des Dirac-Maßes in $\E[X_1]$. Mit Stetigkeitssatz
	\begin{align*}
		\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow[n \to \infty]{\d} \E[X_1].
	\end{align*}
	Und \propref{10_5} gibt
	\begin{align*}
		\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow[n \to \infty]{\P} \E[X_1].
	\end{align*}
\end{proof}
\begin{conclusion}[\person{Cramér}-\person{Wold} device]
	$Y,Y_1,Y_2, \dots$ Zufallsvariablen in $\Rd$. Dann gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\d} Y \Longleftrightarrow \scaProd{u}{Y_n} \xrightarrow[n \to \infty]{\d} \scaProd{u}{Y} \quad \forall u \in \Rd.
	\end{align*}
\end{conclusion}
\begin{proof}
	Es gilt
	\begin{align*}
		\varphi_{\scaProd{u}{Y_n}} (t) = \E[e^{\ii \scaProd{u}{Y_n}}] &= \varphi_{Y_n}(t u)\\
		\intertext{und $\varphi_{\scaProd{u}{Y}}(t) = \varphi_{Y(tu)}$}
		Y_n \xrightarrow[n \to \infty]{\d} Y 
		&\Longleftrightarrow \varphi_{Y_n}(v) \to \varphi_{Y}(v) \quad \forall v \in \Rd \quad \text{Stetigkeitslemma}\\
		&\xLeftrightarrow{\text{s.o.}} \varphi_{\scaProd{u}{Y_n}}(t) \to \varphi_{\scaProd{u}{Y}}(t) \quad \forall t \in \R, u \in \Rd\\
		&\Longleftrightarrow \scaProd{u}{Y_n} \xrightarrow[n \to \infty]{\d} \scaProd{u}{Y} \quad \forall u \in \Rd \quad \text{Stetigkeitslemma}
	\end{align*}
\end{proof}
\begin{proposition}[Lemma von \person{Slutsky}]
	Seien $Y, Y_1, Y_2, \dots$, $Z, Z_1, Z_2, \dots$ $\Rd$-wertige Zufallsvariablen auf $(\O,\F,\P)$, so dass
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\d} Y \und Y_n - Z_n \xrightarrow[n \to \infty]{\P} 0
		\intertext{Dann gilt}
		Z_n \xrightarrow[n \to \infty]{\d} Y
	\end{align*}
\end{proposition}
\begin{proof}
	Es gilt
	\begin{align*}
		\varphi_{Z_n}(u) &= \E[e^{\ii \scaProd{u}{Z_n}}] = \E[e^{\ii \scaProd{u}{Z_n - Y_n}} e^{\ii \scaProd{u}{Y_n}}]\\
		&= \E[(e^{\ii \scaProd{u}{Z_n - Y_n}} - 1)e^{\ii \scaProd{u}{Y_n}}] + \underbrace{\E[e^{\ii \scaProd{u}{Y_n}}]}_{\varphi_{Y_n}(u) \xrightarrow[n \to \infty]{} \varphi_{Y}(u) \quad \forall u \in \Rd}
	\end{align*}
	Es genügt zu zeigen, dass
	\begin{align*}
		E&:= \E[(e^{\ii \scaProd{u}{Z_n - Y_n}} - 1)e^{\ii \scaProd{u}{Y_n}}] \xrightarrow[n \to \infty]{} 0
		\intertext{Dazu}
		\abs{E} &\le \E[\abs{(e^{\ii \scaProd{u}{Z_n - Y_n}} - 1)e^{\ii \scaProd{u}{Y_n}}}]\\
		&= \E[\abs{e^{\ii \scaProd{u}{Z_n - Y_n}} -1}]
	\end{align*}
	Die Funktion $z \mapsto e^{\ii \scaProd{u}{z}}, u \in \Rd$, ist lokal \person{Lipschitz}-stetig, dann für $u,y,z \in \Rd$
	\begin{align*}
		\abs{e^{\ii \scaProd{u}{z}} - e^{\ii \scaProd{u}{y}}} &= \abs{e^{\ii \scaProd{u}{z-y} -1}}\\
		&= \abs{\int_0^{\ii \scaProd{u}{z-y}} e^{\zeta}\d \zeta}\\
		&\le \sup_{\abs{\zeta} \le \abs{\scaProd{u}{z-y}}} \abs{e^{\ii \zeta}} \cdot \abs{\ii \scaProd{u}{z-y}} = \abs{\ii \scaProd{u}{z-y}}\\
		&\le \abs{u} \cdot \abs{z-y}
	\end{align*}
	Damit 
	\begin{align*}
		\abs{E} &\le \E[\abs{e^{\ii \scaProd{u}{Z_n-Y_n}}} \indi_{\set{\abs{Z_n-Y_n} \le \delta}}]\\
		&+ \E[\abs{e^{\ii \scaProd{u}{Z_n-Y_n} -1} \indi_{\set{\abs{Z_n-Y_n} \le \delta}}}]\\
		&\le \delta \abs{u} + 2 \underbrace{\P(\abs{Z_n-Y_n} \le \delta)}_{\xrightarrow[n \to \infty]{}0}\\ %TODO where comes the two from?
		&\xrightarrow[n \to \infty]{} \delta \cdot \abs{u}\\
		&\xrightarrow[\delta \to 0]{} 0.
	\end{align*}
\end{proof}