\begin{definition}[\begriff{schwache Konvergenz}]
	$Y, Y_1, Y_2, \dots$ reelle Zufallsvariablen (``jede ZV darf ihren eigenen WR mitbringen''). Falls für alle $f \in C_b(\R)$ (stetig und beschränkt) gilt, dass
	\begin{align*}
		\lim_{n\to \infty} \E[f(Y_n)] = \E[f(Y)]
	\end{align*} %TODO set \begriff!
	so \emph{konvergiert} $(Y_n)_{n \in \N}$ schwach / in Verteilung gegen $Y$.\\
	Schreibe: $Y_n \konverteil Y$ oder $Y_n \Rightarrow Y, n \to \infty$ oder $\P_{Y_n} \Rightarrow \P_Y$.
\end{definition}
\begin{*remark}
	\begin{itemize}
		\item Formal sollten wir eigentlich schreiben
		\begin{align*}
			\lim \E_n[f(Y_n)] = \E[f(Y)]
		\end{align*}
		wobei $\E_n[f(Y_n)]$ bzgl. $\P_n, Y_n$ auf $(\O_n,\F_n,\P_n)$. 
		Dies wird aber in der Regeln vernachlässigt.
		\item Für Zufallsvariablen in $\Rd$ lässt sich schwache Konvergenz mittels $f \in C_b(\Rd)$ (d.h. $f: \Rd \to \R$) definieren.
	\end{itemize}
\end{*remark}
Der Grenzwert einer schwach konvergenten Folge ist eindeutig \emph{in Verteilung}.
\begin{lemma}
	$Y,Z,Y_1, Y_2, \dots$ reelle Zufallsvariablen, so dass
	\begin{align*}
	Y_n \konverteil Y \und Y_n \konverteil Z
	\end{align*}
	Dann gilt: $Y \distri Z$ bzw. $P_Y = \P_Z$.
\end{lemma}
\begin{proof}
	Betrachte ein fixes kompaktes Intervall $[a,b] \subset \R$. Da die kompakten Intervalle ein $\cap$-stabiler Erzeuger von $\borel(\R)$ sind, genügt es zu zeigen
	\begin{align*}
		\int \indi_{[a,b]} \d \P_Y = \P_Y([a,b]) = \P_Z([a,b]) = \int \indi_{[a,b]} \d \P_Z.
	\end{align*}
	Dazu konstruiere eine Folge $(f_k)_{k \in \N}$ in $C_b(\R)$, so dass $f_k \downarrow f = \indi_{[a,b]}$ ($\nearrow$ Beweis zu \cref{8_13}). Dann folgt mit monotoner Konvergenz
	\begin{align*}
		\int \indi_{[a,b]} \d \P_Y = \lim_{k \to \infty} \int f_k \d \P_Y = \lim_{k \to \infty} \lim_{n \to \infty} \int f_k \d \P_{Y_n}
		\intertext{und analog}
		\int \indi_{[a,b]} \d \P_Z = \lim_{k \to \infty} \lim_{n \to \infty} \int f_k \d \P_{Y_n}
	\end{align*}
	Das liefert die Behauptung.
\end{proof}
\begin{proposition}[\person{Portmanteau}]
	$Y,Y_1,Y_2, \dots$ reelle Zufallsvariablen. Die folgenden Aussagen sind äquivalent:
	\begin{enumerate}
		\item $Y_n \konverteil Y$
		\item $\lim_{n \to \infty} \E[f(Y_n)] = \E[f(Y)] \quad \forall f \in C_b^g(\R)$ (glm stetig und beschränkt)
		\item $\limsup_{n \to \infty} \P(Y_n \in F) \le \P(Y \in F) \quad \forall F \subset \R$ abgeschlossen
		\item $\liminf_{n \to \infty} \P(Y_n \in O) \ge \P(Y \in O) \quad \forall O \subset \R$ offen
		\item $\lim_{n \to \infty} \P(Y_n \in C) = \P(Y \in C) \quad C \in \borel(\R)$ mit $\P_Y(\partial C) = 0$ (Rand von $C$)
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}[label=] %TODO add labels
		\item 1. $\implies$ 2.: ist klar
		\item 2. $\implies$ 3.: Sei $F$ abgeschlossen und definiere für $k \in \N$
		\begin{align*}
			f_k(x) = (1-k \dist (x,F))^+ \mit \dist(x,F) = \inf_{y \in F} \abs{x-y}
		\end{align*}
		Dann ist $f_k$ beschränkt und glm. stetig, denn
		\begin{align*}
			\abs{f_k(y) - f_k(x)} &\le k \abs{\dist(y,F) - \dist(x,F)}\\
			&\le k \abs{y-x} \quad \forall x,y \in \R
			\intertext{da}
			\dist(x,F) &= \inf_z \abs{x-z} \le \inf_z (\abs{x-z} + \abs{y-z})\\
			&= \abs{x-y} + \dist{y,F}.
		\end{align*}
		Zudem gilt $f_k \le \indi_F$ und $f_k \downarrow \indi_F$, so dass
		\begin{align*}
			\limsup_{n \to \infty} \P(Y_n \in F) &= \limsup_{n \to \infty} \E[\indi_F (Y_n)]\\
			&\le \lim_{n \to \infty} \E[f_k(Y_n)]\\
			\over{\text{2.}}&{=} \E[f_k (Y)]
		\end{align*}
		Mit monotoner Konvergenz folgt
		\begin{align*}
			\limsup_{n \to \infty} \P(Y_n \in F) &\le \inf_{k \in \N} \E[f_k(Y)]\\
			&= \E[\indi_F(Y)] = \P(Y \in F)
		\end{align*}
		\item 3. $\implies$ 4.: Für jedes $O \subset \R$ offen ist $O^C$ abgeschlossen, so dass
		\begin{align*}
			\liminf_{n \to \infty} \P(Y_n \in O) &= \liminf_{n \to \infty} (1-\P(Y_n \in O^C))\\
			&= 1- \limsup_{n \to \infty} \P(Y_n \in O^C)\\
			\over{\text{3.}}&{\ge} 1 - \P(Y \in O^C)\\
			&= \P(Y \in O).
		\end{align*}
		\item 4. $\implies$ 3.: Analog und vertausche $\limsup$ mit $\liminf$.
		\item 4. und 3. $\implies$ 5.: Sei $C \in \borel(\R)$ und $overset{\circ}{C}$ dass offene Innere von $C$, $\bar{C} = \overset{\circ}{C} \cup \partial C$ der Abschluss.
		\begin{align*}
			\lim_{n \to \infty} \P(Y_n \in C) &\le \limsup_{n \to \infty} \P(Y_n \in \bar{C})\\
			\overset{\text{3.}}&{\le} \P(Y \in \bar{C})\\
			&= \P(Y \in \overset{\circ}{C}) \quad (\text{ da } \P_Y(\partial C) = 0)\\
			\overset{\text{4.}}&{\le} \liminf_{n \to \infty} \P(Y_n \in \overset{\circ}{C})\\
			&\le \limsup_{n \to \infty} \P(Y_n \in C).
		\end{align*}
		\item 5. $\implies$ 1.: Sei $f \in C_b(\R)$ positiv. (wenn nicht positiv: in positiven und negativen Anteil zerlegen und dann mit Linearität arbeiten). Da $\partial \set{f \ge t}  = \set{f=t}$ gilt, folgt $\P_Y(\partial\set{f \ge t}) > 0$ für höchstens abzählbar viele $t$ und das impliziert 
		\begin{align*}
			\lim_{n \to \infty} \E[f(Y_n)] &= \lim_{n\to \infty} \int_{\R} f \d \P_{Y_n}\\
			&= \lim_{n \to \infty} \int_0^{\infty} \P_{Y_n} (f\ge t)\d t \quad \text{ folgt mit Schilling MINT Satz 16.7}\\
			&= \lim_{n \to \infty} \int_0^{\infty} \P(f(Y_n) \ge t)\d t\\
			&= \int_0^{\infty} \lim_{n \to \infty} \P(f(Y_n) \ge t) \d t \quad \text{dom. Konvergenz}\\
			&= \int_0^{\infty} \P(f(Y) \ge t) \d t \quad \text{ nutze 5.}\\
			&= \E[f(Y)] \quad \text{Satz 16.7}.
		\end{align*}
		Für allgemeines $f$ folgt die Aussage mittels Linearität.
	\end{enumerate}
\end{proof}
\begin{lemma}
	\proplbl{10_4}
	$Y, Y_1, Y_2, \dots$ reelle Zufallsvariablen auf $(\O, \F,\P)$. Es gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\P} \implies Y_n \xrightarrow[n \to \infty]{\d} Y
	\end{align*}
\end{lemma}
\begin{proof}
	Seien $f \in C_b^g(\R)$ und $\epsilon > 0$ fixiert. Betrachte
	\begin{align*}
		\abs{\E[f(Y_n)] - \E[f(Y)]} \le \E[\abs{f(Y_n) - f(Y)}] = \int_{\R} \abs{f(Y_n) - f(Y)}\d \P
	\end{align*}
	Da $f \in C_0^g$ existiert $M \in \R$ mit $\abs{f} \le M$ und 
	\begin{align*}
		\exists \delta = \delta(\epsilon) \in (0,1) \text{ so dass }\forall \abs{x-y}\le \delta \colon \abs{f(x)-f(y)} < \epsilon \tag{$\star$}\label{eq:10_4:a}
	\end{align*}
	Schreibe nun
	\begin{align*}
		\int \abs{f(Y_n)-f(Y)}\d \P = \int_{\set{\abs{Y_n -Y}\le \delta}} ... \D \P + \int_{\set{\abs{Y_n -Y}> \delta}} ... \d \P = E_1 +E_2
	\end{align*}
	mit
	\begin{align*}
		E_1 \overset{\eqref{eq:10_4:a}}&{\le}\epsilon \P(\abs{Y_n - Y} \le \delta)\\
		E_2 &\le 2M \P(\abs{Y_n - Y} > \delta)\\
		E_1 + E_2 &\le \epsilon \P(\abs{Y_n - Y} \le \delta) + 2M \P(\abs{Y_n - Y}> \delta)\\
		&\le \epsilon + 2M \\underbrace{P(\abs{Y_n -Y} > \delta)}_{\xrightarrow[n \to \infty]{}0} \to \epsilon
	\end{align*}
	Für $\epsilon \to 0$ folgt die Behauptung.
\end{proof}
\begin{lemma}
	$Y,Y_1,Y_2,\dots$ reelle Zufallsvariablen auf $(\O,\F,\P)$. Für $c \in \R$ konstant gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\d} Y \equiv c \Longleftrightarrow Y_n \xrightarrow[n \to \infty]{\P} Y \equiv c
	\end{align*}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item $\Leftarrow$: \propref{10_4}
		\item $\Rightarrow$: Für $\epsilon > 0$ fixiert, wähle $f \in C_b(\R)$ so dass
		\begin{align*}
			f(0) = 0 &\quad f(x) \ge \indi_{[-\epsilon,\epsilon]^C}(x) \quad \forall x \in \R
			\intertext{Dann}
			\P(\abs{Y_n - Y}> \epsilon) &= \int_{[-\epsilon,\epsilon]^C} (Y_n - Y)\d \P\\
			&\le \int F(Y_n-Y)\d \P\\
			&= \int f(Y_n - c)\d \P\\
			&\xrightarrow[n\to \infty]{} \int f(Y-c) \d \P\\
			&= \int f(0)\d \P = 0.
		\end{align*}
	\end{itemize}
\end{proof}
\begin{proposition}
	$Y,Y_1,Y_2,\dots$ reelle Zufallsvariablen mit Verteilungsfunktionen $F_Y,F_{Y_1},F_{Y_2}, \dots$.
	Dan gilt $Y_n \xrightarrow[n \to \infty]{\d} Y$ genau dann wenn
	\begin{align*}
		\lim_{n \to \infty} F_{Y_n} = F_Y(x) \quad \forall \text{ Stetigkeitsstellen $x$ von $F_Y$}.
	\end{align*}
	Ist $F_Y$ stetig, so gilt in diesem Fall sogar gleichmäßige Konvergenz
	\begin{align*}
		\lim_{n \to \infty} \sup_{x \in \R} \abs{F_{Y_n}(x) - F_Y(x)} = 0.
	\end{align*}
\end{proposition}
\begin{proof}
	Es gelte $Y_n \xrightarrow[n\to \infty]{\d} Y$, d.h. $\E[f(Y_n)]\to \E[f(Y)]\; \forall f \in C_b(\R)$. Sei $x \in \R$ fixiert, $\epsilon > 0$ und wähle $f \in C_b(\R)$ mit
	\begin{align*}
		\indi_{(-\infty,x]} &\le f \le \indi_{(-\infty,x+\epsilon]}
		\intertext{Dann}
		\limsup_{n \to \infty} F_{Y_n}(x) &= \limsup_{n \to \infty} \P(Y_n \le x)\\
		&= \limsup_{n \to \infty} \E[\indi_{(-\infty,x]} (Y_n)]\\
		&\le \limsup_{n \to \infty} \E[f(Y_n)]\\
		&= \lim \E[f(Y_n)] = \E[f(Y)]\\
		&\le \E[\indi_{(-\infty,x+\epsilon]}(Y)] = F_Y(x+\epsilon)
	\end{align*}
	Für $\epsilon \to 0$ folgt, da $F_Y$ rechtsstetig
	\begin{align*}
		\limsup_{n \to \infty} F_{Y_n} (x) \le F_Y(x)
	\end{align*}
	Ist $x$ Stetigkeitsstelle von $F_Y$, so gilt auch $F_Y(x-\epsilon) \to F_Y(x), \epsilon \to 0$ und eine analoge Rechnung zeigt
	\begin{align*}
		\liminf_{n \to \infty} F_{Y_n}(x) \ge F_Y(x)
		\intertext{also}
		\lim F_{Y_n}(x) = F_Y(x)
	\end{align*}
	Umgekehrt: Es gelte $\lim_{n \to \infty} F_{Y_n}(x) = F_Y(x)$ $\forall$ Stetigkeitsstellen $x$ von $F_Y$. Fixiere $f \in C_b^g(\R)$ (\person{Portmanteau}!), $\epsilon > 0$ und wähle Stetigkeitsstellen $x_1 < x_2 < ... x_n$ von $F_Y$, so dass
	\begin{align*}
		F_Y(x_1) < \epsilon, F_Y(x_k) > 1 -\epsilon \und\\
		\abs{f(y)-f(x_i)} < \epsilon \quad \forall x_{i-1} \le y \le x_i
	\end{align*}
	(möglich, da $f$ gleichmäßig stetig und $F_Y$ wegen Monotonie nur abzählbar viele Unstetigkeitsstellen besitzt.) Das impliziert
	\begin{align*}
		\E[f(Y_n)] &= \sum_{i=2}^k \E[f(y_n)\indi_{\set{x_{i-1} < Y_n \le x_i}}] + \E[f(Y_n)\indi_{\set{Y_n \le x_1}\cup \set{Y_n > x_k}}]\\
		&\le \sum_{i=2}^k (f(x_i)+\epsilon)(F_{Y_n}(x_i) - F_{Y_n}(x_{i-1})) + 2\norm{f}_{\infty}\epsilon\\
		&\xrightarrow[n \to \infty]{} \sum_{i=2}^k (f(x_i)+ \epsilon)(F_Y(x_i)-F_Y(x_{i-1}) + 2\norm{f}_{\infty}\epsilon\\
		&\le \E[f(Y)] + 2\epsilon(1+2\norm{f}_{\infty})
	\end{align*}
	Für $\epsilon \to 0$ folgt
	\begin{align*}
		\limsup_{n \to \infty} \E[f(Y_n)] \le \E[f(Y)]
	\end{align*}
	Der $\liminf$ folgt analog
	\begin{align*}
		\lim \E[f(Y_n)] = \E[f(Y)].
	\end{align*}
	Zur gleichmäßigen Konvergenz: Sei $F_Y$ stetig und $\epsilon = k^{-1}, k \in \N$. Dann existieren $z_i \in \R$ mit
	\begin{align*}
		F_Y(z_i) = \frac{i}{k} \quad 0 < i < k
	\end{align*}
	Da auch $F_{Y_n}$ monoton wächst, gilt
	\begin{align*}
		\sup_{x \in \R} \abs{F_{Y_n}(x) -F_Y(x)} \le \epsilon + \underbrace{\max_{0 \le i \le k}\abs{F_{Y_n}(z_i)-F_Y(z_i)}}_{\xrightarrow[n \to \infty]{}0}.
	\end{align*}
	%TODO tikzpic check ERIC!
\end{proof}
\begin{proposition}[Stetigkeitssatz]
	\proplbl{10_7}
	$Y, Y_1, Y_2, \dots$ Zufallsvariablen in $\Rd$ mit charakteristischen Funktionen $\varphi_Y, \varphi_{Y_1}, \varphi_{Y_2}, \dots$. Dann gilt
	\begin{align*}
		Y_n \xrightarrow[n \to \infty]{\d} Y \Longleftrightarrow \lim_{n \to \infty} \varphi_{Y_n}(u) = \varphi_Y(u) \quad \forall u \in \Rd
	\end{align*} 
\end{proposition}
Für den Beweis benötigen wir:
\begin{lemma}
	\proplbl{10_8}
	$Y,Y_1,Y_2, \dots$ Zufallsvariablen in $\Rd$ und sei $N = (N_1, \dots, N_d)^T$ Zufallsvektor so dass $N_1, N_2, \dots, N_d$ u.i.v. mit $N_i \sim \normal(0,1)$, unabhängig von $Y,Y_1, Y_2, \dots$ Falls:
	\begin{align*}
		Y_n + \sigma N \xrightarrow[n \to \infty]{\d} Y + \sigma N \quad \sigma > 0
		\intertext{dann folgt}
		Y_n \xrightarrow[n \to \infty]{\d} Y
	\end{align*}
\end{lemma}
\begin{proof}
	Sei $f \in C_b^g(\R)$ und $\epsilon > 0$ beliebig.
	\begin{align*}
		\abs{\E[f(Y_n)-f(Y)]} &\le \abs{\E[f(Y_n)] - \E[f(Y_n + \sigma N)]}\\
		&+ \underbrace{\abs{\E[f(Y_n + \sigma N)] - \E[f(Y+\sigma N)]}}_{\xrightarrow[n \to \infty]{}0}\\
		&+ \abs{\E[f(Y+\sigma N)] - \E[f(Y)]}
	\end{align*}
	Da $f \in C_b^g(\R)$ existiert $\delta > 0$, so dass
	\begin{align*}
		\abs{f(x)-f(y)} \le \epsilon \text{ für } \abs{x-y} \le \delta
	\end{align*}
	Damit
	\begin{align*}
		\abs{\E[f(Y_n)] - \E[f(Y_n + \sigma N)]} &= \abs{\E[\underbrace{f(Y_n) - f(Y_n + \sigma N)}_{\le \epsilon \mit \norm{\sigma N}< \delta}]}\\
		&\le \epsilon + 2 \norm{f}_{\infty}\P(\norm{N} > \frac{\delta}{\sigma}) \le 2 \epsilon \quad \text{ für } \sigma \le \sigma_0(\delta, \epsilon), \forall n \in \N
	\end{align*}
	Dann folgt für $n$ hinreichend groß $\abs{\E[f(Y_n)] - \E[f(Y)]} \le 5 \epsilon$. Daraus folgt die Behauptung.
\end{proof}